{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "# Function to create image array stacks from given timestamps and locations\n",
    "def makeStacks(path,timeStamps,channels=['94','131','171','193','211','304','335'],initCoords=[0*u.arcsec,0*u.arcsec],ROIsize=300*u.arcsec,rotationCorr=1,threads=1,clusterScale=16,addNoise=0,level1Noise=0):\n",
    "    ### Create the cluster\n",
    "    import dask\n",
    "    import distributed\n",
    "    from dask.distributed import Client, LocalCluster\n",
    "    cluster = LocalCluster(threads_per_worker=threads)\n",
    "    cluster.scale(clusterScale)\n",
    "    client = Client(cluster)\n",
    "    \n",
    "    import astropy.units as u\n",
    "    import sunpy.map\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import drms\n",
    "    from astropy.coordinates import SkyCoord\n",
    "    from sunpy.util.metadata import MetaDict\n",
    "    import sunpy.io.fits\n",
    "    from sunpy.map import Map\n",
    "    import os \n",
    "    from sunpy.physics import solar_rotation\n",
    "    import astropy.time\n",
    "    from astropy.visualization import ImageNormalize, SqrtStretch, time_support\n",
    "    from aiapy.calibrate import correct_degradation\n",
    "    from aiapy.calibrate.util import get_correction_table\n",
    "    from aiapy.calibrate import register, update_pointing\n",
    "    \n",
    "    ### Get correction table\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    correction_table = get_correction_table(correction_table='/ssw/sdo/aia/response/aia_V8_20171210_050627_response_table.txt')\n",
    "    import warnings\n",
    "    warnings.filterwarnings('default')\n",
    "    \n",
    "    ### Get images\n",
    "    client_drms = drms.Client()\n",
    "    keys_aia_tot = []\n",
    "    paths_aia_tot = []\n",
    "    for j in range(len(timeStamps[:])):\n",
    "        keys_aia = []\n",
    "        paths_aia = []\n",
    "        for i in range(len(channels)):\n",
    "          keys,paths = client_drms.query(\n",
    "              'aia.lev1_euv_12s['+timeStamps[j]+']['+channels[i]+']',\n",
    "              key=drms.const.all,\n",
    "              seg='image',\n",
    "          )\n",
    "          keys_aia.append(keys)\n",
    "          paths_aia.append(paths)\n",
    "        keys_aia_tot.append(keys_aia)\n",
    "        paths_aia_tot.append(paths_aia)\n",
    "    \n",
    "    ### Define the Dask function for distributed computing\n",
    "    def getFrame(args):\n",
    "\n",
    "      i = args[0]\n",
    "      timeStamp = args[1]\n",
    "      x0 = args[2][0]\n",
    "      y0 = args[2][1]\n",
    "      ROIsize = args[3]\n",
    "      correction_table = args[4]\n",
    "      path = args[5]\n",
    "      keys = args[6]\n",
    "      paths = args[7]\n",
    "      m_aia_init = args[8]\n",
    "      rotationCorr = args[9]\n",
    "      addNoise = args[10]\n",
    "      level1Noise = args[11]\n",
    "        \n",
    "      stack= []\n",
    "      noisey_stack = []\n",
    "      level_1_noise = []\n",
    "      for j in range(len(channels)):\n",
    "          m_aia = sunpy.map.Map(paths[i][j]['image'][0])\n",
    "        \n",
    "          m_normalized_aia = sunpy.map.Map(\n",
    "            m_aia.data/m_aia.exposure_time.to(u.s).value,\n",
    "            m_aia.meta\n",
    "          )\n",
    "            \n",
    "          # Create noisey map\n",
    "          if addNoise == 1:\n",
    "              noise_data = m_aia.data\n",
    "              noise_meta = m_aia.meta\n",
    "              # Add poisson noise for positves and negatives\n",
    "              np.random.seed(0)\n",
    "              noise_data = noise_data+(np.abs(noise_data)-np.random.poisson(np.abs(noise_data)))\n",
    "              m_noise_aia = sunpy.map.Map(noise_data,noise_meta)\n",
    "              m_normalized_noise_aia = sunpy.map.Map(\n",
    "                m_noise_aia.data/m_noise_aia.exposure_time.to(u.s).value,\n",
    "                m_noise_aia.meta\n",
    "              )\n",
    "\n",
    "              # Isolate level 1 noise for analysis\n",
    "              if level1Noise == 1:\n",
    "                  level_1_placeholder = m_normalized_aia\n",
    "                  level_1_noise_placeholder = m_normalized_noise_aia    \n",
    "            \n",
    "          # Back to original map\n",
    "          m_updated_pointing_aia = update_pointing(m_normalized_aia)\n",
    "          m_registered_aia = register(m_updated_pointing_aia)\n",
    "          m_corrected_aia = correct_degradation(m_registered_aia,correction_table=correction_table)\n",
    "          maps_frame_aia = m_corrected_aia\n",
    "          #maps_frame_aia = m_registered_aia\n",
    "          ###\n",
    "          # Adjust for shift in images and apply it to x0 and y0\n",
    "          ###\n",
    "          if rotationCorr == 1:\n",
    "              adjustMaps = sunpy.map.Map(m_aia_init,maps_frame_aia,sequence=True)\n",
    "              adjust = sunpy.physics.solar_rotation.calculate_solar_rotate_shift(adjustMaps)\n",
    "              x0 = x0 - adjust['x'][1]\n",
    "              y0 = y0 - adjust['y'][1]\n",
    "          ###\n",
    "          bottom_left_aia = SkyCoord(x0 - ROIsize, y0 - ROIsize,\n",
    "                               frame=maps_frame_aia.coordinate_frame)\n",
    "          top_right_aia = SkyCoord(x0 + ROIsize, y0 + ROIsize,\n",
    "                             frame=maps_frame_aia.coordinate_frame)\n",
    "          submap_frame_aia = maps_frame_aia.submap(bottom_left_aia,top_right_aia)\n",
    "                                                                                    \n",
    "          # Back to noisey map    \n",
    "          if addNoise == 1:\n",
    "              m_updated_pointing_noise_aia = update_pointing(m_normalized_noise_aia)\n",
    "              m_registered_noise_aia = register(m_updated_pointing_noise_aia)\n",
    "              m_corrected_noise_aia = correct_degradation(m_registered_noise_aia,correction_table=correction_table)\n",
    "              maps_frame_noise_aia = m_corrected_noise_aia\n",
    "\n",
    "              bottom_left_noise_aia = SkyCoord(x0 - ROIsize, y0 - ROIsize,\n",
    "                                   frame=maps_frame_noise_aia.coordinate_frame)\n",
    "              top_right_noise_aia = SkyCoord(x0 + ROIsize, y0 + ROIsize,\n",
    "                                 frame=maps_frame_noise_aia.coordinate_frame)\n",
    "              submap_frame_noise_aia = maps_frame_noise_aia.submap(bottom_left_noise_aia,top_right_noise_aia)\n",
    "\n",
    "              # Create submaps for level 1 noise, we are doing this down here so x0 and y0 can be updated if rotation is turned on\n",
    "              if level1Noise == 1:\n",
    "                  xScale = level_1_placeholder.scale[0].value\n",
    "                  yScale = level_1_placeholder.scale[1].value\n",
    "                  xCenterPixel = np.floor(x0.value/xScale)+(level_1_placeholder.data.data.shape[0]/2)\n",
    "                  yCenterPixel = np.floor(y0.value/yScale)+(level_1_placeholder.data.data.shape[0]/2)\n",
    "                  # Cut to same size across channels so they can be stacked, the actual arcsec FOV doesn't matter since this will be used for a histogram  \n",
    "                  cutSizeX = submap_frame_aia.data.shape[0]\n",
    "                  cutSizeY = submap_frame_aia.data.shape[1]\n",
    "                  if cutSizeX%2 == 1:\n",
    "                    leftX = xCenterPixel-(cutSizeX-1)/2\n",
    "                    rightX = xCenterPixel+(cutSizeX-1)/2+1\n",
    "                  else:\n",
    "                    leftX = xCenterPixel-(cutSizeX)/2\n",
    "                    rightX = xCenterPixel+(cutSizeX)/2\n",
    "                  if cutSizeY%2 == 1:\n",
    "                    bottomY = yCenterPixel-(cutSizeY-1)/2\n",
    "                    topY = yCenterPixel+(cutSizeY-1)/2+1\n",
    "                  else:\n",
    "                    bottomY = yCenterPixel-(cutSizeY)/2\n",
    "                    topY = yCenterPixel+(cutSizeY)/2\n",
    "                  lvl1_submap = level_1_placeholder.data[int(leftX):int(rightX),int(bottomY):int(topY)]\n",
    "                  lvl1_noise_submap = level_1_noise_placeholder.data[int(leftX):int(rightX),int(bottomY):int(topY)]\n",
    "\n",
    "          # Compile stacks\n",
    "          if addNoise == 1:\n",
    "              noisey_stack.append(submap_frame_noise_aia.data)\n",
    "              if level1Noise == 1:\n",
    "                  level_1_noise.append(lvl1_noise_submap-lvl1_submap)  \n",
    "          stack.append(submap_frame_aia.data)\n",
    "      if addNoise == 1 and level1Noise == 1:\n",
    "          return np.stack(stack),np.stack(noisey_stack),np.stack(level_1_noise) \n",
    "      elif addNoise == 1:\n",
    "          return np.stack(stack),np.stack(noisey_stack);\n",
    "      else:\n",
    "          return np.stack(stack)\n",
    "      \n",
    "    ### Create loop for Dask function\n",
    "    m_aia_init0 = sunpy.map.Map(paths_aia_tot[0][0]['image'][0])\n",
    "    m_updated_pointing_aia_init = update_pointing(m_aia_init0)\n",
    "    m_aia_init  = register(m_updated_pointing_aia_init)\n",
    "    lazy_results = []\n",
    "    for i in range(len(timeStamps)):\n",
    "      args = [i,timeStamps[i],initCoords,ROIsize,correction_table,path,keys_aia_tot,paths_aia_tot,m_aia_init,rotationCorr,addNoise,level1Noise]\n",
    "      lazy = dask.delayed(getFrame)(args)\n",
    "      lazy_results.append(lazy)\n",
    "    ### Execute function\n",
    "    stacks = dask.compute(lazy_results)\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    if addNoise == 1 and level1Noise == 1:\n",
    "        return np.squeeze(np.stack(stacks[0],axis=1))[0],np.squeeze(np.stack(stacks[0],axis=1))[1],np.squeeze(np.stack(stacks[0],axis=1))[2]\n",
    "    elif addNoise == 1:\n",
    "        return np.squeeze(np.stack(stacks[0],axis=1))[0],np.squeeze(np.stack(stacks[0],axis=1))[1];\n",
    "    else:\n",
    "        return np.stack(stacks[0],axis=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: SunpyUserWarning: Input data has been cast to float64. [sunpy.image.transform]\n",
      "WARNING: SunpyUserWarning: Input data has been cast to float64. [sunpy.image.transform]\n",
      "WARNING: SunpyUserWarning: Input data has been cast to float64. [sunpy.image.transform]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# You need aiapy, sunpy, graphviz instaled\n",
    "###\n",
    "import astropy.units as u\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path = '/Users/penwarden/Desktop/'\n",
    "\n",
    "### Get training data\n",
    "addNoise = 1\n",
    "tStart = ''\n",
    "tLength = '1h'\n",
    "tCadence = '1440s'\n",
    "trainStamps = ['2010-07-15T00:00:00','2010-07-16T00:00:00','2010-06-14T00:00:00','2010-06-15T00:00:00']\n",
    "rotationCorr = 0\n",
    "ROIsize=500*u.arcsec\n",
    "initCoords = [51*u.arcsec,248*u.arcsec]\n",
    "#threads = 1\n",
    "#clusterScale = 16\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "trainStack, trainNoiseStack = makeStacks(path,trainStamps,initCoords=initCoords,rotationCorr=rotationCorr,ROIsize=ROIsize,addNoise=addNoise)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "### Get recent testing data without noise\n",
    "initCoords = [-44*u.arcsec,-49*u.arcsec]\n",
    "initCoords = [-400*u.arcsec,-400*u.arcsec]\n",
    "testStamps = ['2017-08-26T02:52:58','2018-08-26T00:00:00'] # AR & QS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "testStack2017 = makeStacks(path,testStamps,initCoords=initCoords,rotationCorr=rotationCorr,ROIsize=ROIsize)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "### Get old testing data with noise\n",
    "initCoords = [83*u.arcsec,126*u.arcsec]\n",
    "testStamps = ['2010-08-15T00:00:00','2011-01-28T00:00:00'] # AR & QS\n",
    "addNoise = 1\n",
    "level1Noise = 1\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "testStack, testNoiseStack, level_1_testNoise = makeStacks(path,testStamps,initCoords=initCoords,rotationCorr=rotationCorr,ROIsize=ROIsize,addNoise=addNoise,level1Noise = level1Noise)\n",
    "import warnings\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function (2 images with 3 sets of channels each)\n",
    "def stackPlotter(names,stacks,file_name,histName,saveFigure,bins,pathDir,testType = 1,channels=['$_{94}$','$_{131}$','$_{171}$','$_{193}$','$_{211}$','$_{304}$','$_{335}$'],cmaps=['sdoaia94','sdoaia131','sdoaia171','sdoaia193','sdoaia211','sdoaia304','sdoaia335'],lossWeights = [1,1,1,1,1,1,1]):\n",
    "    import matplotlib.pyplot as plt\n",
    "    font = {'fontname':'Helvetica'}\n",
    "    cmapNoise = ['seismic','seismic','seismic','seismic','seismic','seismic','seismic']\n",
    "    chanNum = len(channels)\n",
    "    plt.figure(figsize=(5*len(channels),6*(len(names)+2)));\n",
    "    histColor = ['blue','orange','green']\n",
    "    annotate = np.empty((2,chanNum,3),dtype='U100')\n",
    "    for i in range(len(names)):\n",
    "        for j in range(len(channels)): \n",
    "            histCount = int(np.floor(i/3)) # index for which 3 rows go to which histogram\n",
    "            \n",
    "            mean = np.mean(stacks[i][j])\n",
    "            std = np.std(stacks[i][j])\n",
    "            median = np.median(stacks[i][j])\n",
    "            centerThree = (histCount)*3+1 # center index of groups of three\n",
    "            listMean = [np.mean(stacks[centerThree-1][j]),np.mean(stacks[centerThree][j]),np.mean(stacks[centerThree+1][j])]\n",
    "            listSTD = [np.std(stacks[centerThree-1][j]),np.std(stacks[centerThree][j]),np.std(stacks[centerThree+1][j])]\n",
    "            xmax = np.max(np.concatenate([stacks[centerThree-1][j].flatten(),stacks[centerThree][j].flatten(),stacks[centerThree+1][j].flatten()]))\n",
    "            xmin = np.min(np.concatenate([stacks[centerThree-1][j].flatten(),stacks[centerThree][j].flatten(),stacks[centerThree+1][j].flatten()]))\n",
    "            \n",
    "            plt.subplot(len(names)+2,chanNum,i*chanNum+histCount*chanNum+j+1)\n",
    "            if testType == 0 and i == centerThree+1:\n",
    "                plt.imshow(stacks[i][j],cmap=cmapNoise[j],vmin = -5, vmax = 5,origin = 'lower') # Set abs(vmin) = vmax to center cmap at 0\n",
    "            else:\n",
    "                plt.imshow(stacks[i][j],cmap=cmaps[j],vmin = xmin,vmax = xmax,origin = 'lower')\n",
    "            plt.title(names[i]+' '+channels[j],fontsize=25, **font)\n",
    "            plt.xlabel('Pixel',fontsize=10, **font)\n",
    "            plt.ylabel('Pixel',fontsize=10, **font);\n",
    "\n",
    "            plt.subplot(len(names)+2,chanNum,(histCount+1)*3*chanNum+j+1+chanNum*histCount)\n",
    "            plt.hist(stacks[i][j].flatten(),bins=bins,alpha=0.5,density=True,range=[xmin,xmax])\n",
    "            plt.title('Histograms',fontsize=25, **font)\n",
    "            plt.xlabel('Pixel Value',fontsize=10),plt.ylabel('Count',fontsize=10, **font);\n",
    "            plt.gca().set_yscale(\"log\");\n",
    "            annotate[histCount][j][int(i%3)] = histName[int(i%3)]+'\\nmean: '+str(round(mean,2))+'\\nstd: '+str(round(std,2))+'\\nmedian: '+str(round(median,2));\n",
    "            plt.xlim([np.max(listMean)-7*np.max(listSTD),np.max(listMean)+21*np.max(listSTD)]);\n",
    "    # Add legends\n",
    "    for i in range(0,2):\n",
    "        for j in range(len(channels)):\n",
    "            plt.subplot(len(names)+2,chanNum,(i+1)*3*chanNum+j+1+chanNum*i)\n",
    "            plt.legend([annotate[i][j][0],annotate[i][j][1],annotate[i][j][2]])\n",
    "    if saveFigure == 1:\n",
    "        plt.savefig(pathDir+'/'+file_name+'.png');\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def shapeStack (stack,scaleStack):\n",
    "    shapeInput = [len(stack),stack[0].shape[0]*stack[0].shape[1]]\n",
    "    scaled = []\n",
    "    for i in range(len(stack)):\n",
    "        scaled.append(stack[i]/np.median(scaleStack[i]))\n",
    "    scaledData = np.transpose(np.reshape(np.stack(scaled),shapeInput))\n",
    "    \n",
    "    return scaled, scaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data used for scaling\n",
    "scaleStack = np.mean(trainStack,0)\n",
    "#np.save('scaleStack',scaleStack)\n",
    "\n",
    "# Get train data in correct NN format\n",
    "cleanTrain = [];cleanTrainData = [];noiseTrain = [];noiseTrainData =[]\n",
    "for i in range(trainStack.shape[0]):\n",
    "    clean, cleanData = shapeStack(trainStack[i],scaleStack)\n",
    "    noise, noiseData = shapeStack(trainNoiseStack[i],scaleStack)\n",
    "    \n",
    "    cleanTrain.append(clean),cleanTrainData.append(cleanData),noiseTrain.append(noise),noiseTrainData.append(noiseData) \n",
    "    \n",
    "# Define NN shape\n",
    "NNshape = [cleanTrainData[0].shape[0]*len(cleanTrainData),cleanTrainData[0].shape[1]]\n",
    "\n",
    "cleanTrain_NN = np.reshape(np.stack(cleanTrainData),NNshape)\n",
    "noiseTrain_NN = np.reshape(np.stack(noiseTrainData),NNshape)\n",
    "cleanTrain_image = np.stack(cleanTrain)\n",
    "noiseTrain_image = np.stack(noiseTrain)\n",
    "\n",
    "# Get test data in correct NN format\n",
    "cleanTest = [];cleanTestData = [];noiseTest = [];noiseTestData =[]\n",
    "for i in range(testStack.shape[0]):\n",
    "    clean, cleanData = shapeStack(testStack[i],scaleStack)\n",
    "    noise, noiseData = shapeStack(testNoiseStack[i],scaleStack)\n",
    "    \n",
    "    cleanTest.append(clean),cleanTestData.append(cleanData),noiseTest.append(noise),noiseTestData.append(noiseData) \n",
    "\n",
    "cleanTest_NN = np.stack(cleanTestData)\n",
    "noiseTest_NN = np.stack(noiseTestData)\n",
    "cleanTest_image = np.stack(cleanTest)\n",
    "noiseTest_image = np.stack(noiseTest)\n",
    "\n",
    "# Get recent test data in correct NN format\n",
    "cleanTestRecent = [];cleanTestRecentData = []\n",
    "for i in range(testStack.shape[0]):\n",
    "    clean, cleanData = shapeStack(testStack2017[i],scaleStack)\n",
    "    cleanTestRecent.append(clean),cleanTestRecentData.append(cleanData)\n",
    "\n",
    "cleanTestRecent_NN = np.stack(cleanTestRecentData)\n",
    "cleanTestRecent_image = np.stack(cleanTestRecent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(layers,activationFunc,regCoeff):\n",
    "    # Set seeds for consitant results\n",
    "    seed_value= 3\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "    import numpy as np\n",
    "    np.random.seed(seed_value)\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    import keras\n",
    "    from keras.layers import Input, Dense, Dropout, LeakyReLU, Add\n",
    "    from keras.models import Model\n",
    "\n",
    "    ### Create Autoencoder model\n",
    "    input_pixel = Input(shape=(7,))\n",
    "    \n",
    "    if activationFunc == 'LeakyReLU':\n",
    "        layer = Dense(units=7,activation=LeakyReLU(alpha=0.1),kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros')(input_pixel)\n",
    "        for i in range(len(layers)):\n",
    "            layer = Dense(units=layers[i],activation=LeakyReLU(alpha=0.1),kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',activity_regularizer=keras.regularizers.l1(regCoeff))(layer)\n",
    "        \n",
    "    elif activationFunc == 'Linear':\n",
    "        layer = Dense(units=7,kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros')(input_pixel)\n",
    "        for i in range(len(layers)):\n",
    "            layer = Dense(units=layers[i],kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',activity_regularizer=keras.regularizers.l1(regCoeff))(layer)\n",
    "        \n",
    "    else:\n",
    "        layer = Dense(units=7,activation='relu',kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros')(input_pixel)\n",
    "        for i in range(len(layers)):\n",
    "            layer = Dense(units=layers[i],activation='relu',kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',activity_regularizer=keras.regularizers.l1(regCoeff))(layer)\n",
    "\n",
    "    output = Dense(units=7,kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros')(layer)\n",
    "    \n",
    "    merge = Add()([output,input_pixel])\n",
    "    autoencoder = keras.Model(input_pixel,merge)\n",
    "    autoencoder.summary()\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(autoencoder,trainIn,trainOut,lossWeights,batch_size,epochs,lossFunction,learningRate,validation_split=0.2):\n",
    "    import keras\n",
    "    import keras.backend as K  \n",
    "    # Define custom MSE loss function with weights on channels\n",
    "    def customLossWrapper(lossWeights,lossFunction):\n",
    "        def customLoss(yTrue,yPred):\n",
    "            if lossFunction == 'MSE':\n",
    "                return K.mean(K.square(yTrue-yPred)*lossWeights,axis=-1) #MSE\n",
    "            elif lossFunction =='L1':\n",
    "                return K.mean(K.abs(yTrue-yPred)*lossWeights,axis=-1) #L1\n",
    "        return customLoss\n",
    "    \n",
    "    # Train model\n",
    "    optimizer = keras.optimizers.Adam(lr = learningRate)\n",
    "    #optimizer = keras.optimizers.SGD(lr = learningRate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #autoencoder.compile(optimizer=optimizer,loss=customLossWrapper(lossWeights,lossFunction))\n",
    "    autoencoder.compile(optimizer=optimizer,loss=keras.losses.MeanAbsoluteError())\n",
    "    \n",
    "    history = autoencoder.fit(trainIn,trainOut,epochs=epochs,batch_size=batch_size,validation_split=validation_split,shuffle=True)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelArtificial(autoencoder,cleanTest_NN,cleanTest_image,noiseTest_NN,noiseTest_image):\n",
    "    # Create shape to revert from NN format to image format\n",
    "    shapeOutput = [len(testStack[0]),testStack[0][0].shape[0],testStack[0][0].shape[1]]\n",
    "\n",
    "    # Use trained model to denoise test images\n",
    "    denoiseTest_image = []\n",
    "    predictedTest = autoencoder.predict(noiseTest_NN[0])\n",
    "    denoiseTest_image.append(np.reshape(np.transpose(predictedTest),shapeOutput))\n",
    "    predictedTest = autoencoder.predict(noiseTest_NN[1])\n",
    "    denoiseTest_image.append(np.reshape(np.transpose(predictedTest),shapeOutput))\n",
    "    denoiseTest_image = np.stack(denoiseTest_image)\n",
    "\n",
    "    # Isolate added and removed noise\n",
    "    denoiseOnly = []\n",
    "    denoiseOnly.append(np.stack(noiseTest_image)[0]-denoiseTest_image[0])\n",
    "    denoiseOnly.append(np.stack(noiseTest_image)[1]-denoiseTest_image[1])\n",
    "    denoiseOnly = np.stack(denoiseOnly)\n",
    "\n",
    "    noiseOnly = []\n",
    "    noiseOnly.append(np.stack(noiseTest_image)[0]-np.stack(cleanTest_image)[0])\n",
    "    noiseOnly.append(np.stack(noiseTest_image)[1]-np.stack(cleanTest_image)[1])\n",
    "    noiseOnly = np.stack(noiseOnly)\n",
    "    return denoiseOnly, noiseOnly, denoiseTest_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelUnmodified(autoencoder,cleanTest_NN,cleanTest_image):\n",
    "    # Create shape to revert from NN format to image format\n",
    "    shapeOutput = [len(testStack[0]),testStack[0][0].shape[0],testStack[0][0].shape[1]]\n",
    "\n",
    "    # Use trained model to denoise test images\n",
    "    denoiseTest_image = []\n",
    "    predictedTest = autoencoder.predict(cleanTest_NN[0])\n",
    "    denoiseTest_image.append(np.reshape(np.transpose(predictedTest),shapeOutput))\n",
    "    predictedTest = autoencoder.predict(cleanTest_NN[1])\n",
    "    denoiseTest_image.append(np.reshape(np.transpose(predictedTest),shapeOutput))\n",
    "    denoiseTest_image = np.stack(denoiseTest_image)\n",
    "\n",
    "    # Isolate added and removed noise\n",
    "    denoiseOnly = []\n",
    "    denoiseOnly.append(np.stack(cleanTest_image)[0]-denoiseTest_image[0])\n",
    "    denoiseOnly.append(np.stack(cleanTest_image)[1]-denoiseTest_image[1])\n",
    "    denoiseOnly = np.stack(denoiseOnly)\n",
    "    \n",
    "    return denoiseOnly, denoiseTest_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo the scale applied for the NN\n",
    "def rescale (stack,scaleStack):\n",
    "    newStack = []\n",
    "    for i in range(len(stack)):\n",
    "        newStack.append(stack[i]*np.median(scaleStack[i]))\n",
    "    return np.stack(newStack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chiSquare (data_1,data_2,labels,title,bins=100):\n",
    "    # Get hist values for chi square\n",
    "    hist_1, bin_edges_1 = np.histogram(data_1,bins=bins,density=True,range=[data_1.mean()-3*data_1.std(),data_1.mean()+3*data_1.std()])\n",
    "    hist_2, bin_edges_1 = np.histogram(data_2,bins=bins,density=True,range=[data_2.mean()-3*data_2.std(),data_2.mean()+3*data_2.std()])\n",
    "              # Chi square Test\n",
    "    s, p = scipy.stats.chisquare(hist_1,hist_2)\n",
    "    plt.legend(labels);\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Pixel Value');\n",
    "    plt.title(title+': '+'Chi2 = '+str(s)+' & p = '+str(p));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChi2(data_1,data_2):\n",
    "    import scipy\n",
    "    #d1 = data_1.flatten()\n",
    "    #d2 = data_2.flatten()\n",
    "    min_1 = data_1.mean()-3*data_1.std()\n",
    "    max_1 = data_1.mean()+3*data_1.std()\n",
    "    min_2 = data_2.mean()-3*data_2.std()\n",
    "    max_2 = data_2.mean()+3*data_2.std()\n",
    "    minimum = np.min([min_1,min_2])\n",
    "    maximum = np.max([max_1,max_2])\n",
    "    width = np.abs(minimum-maximum)\n",
    "    bins = int(np.ceil(width/0.25)) # Accuracy should be no greater then 0.25 DN because of quantization\n",
    "    hist_1, bin_edges_1 = np.histogram(data_1,bins=bins,density=True,range=[min_1,max_1])\n",
    "    hist_2, bin_edges_2 = np.histogram(data_2,bins=bins,density=True,range=[min_2,max_2])\n",
    "    s, p = scipy.stats.chisquare(hist_1,hist_2)\n",
    "    #chi2 = np.sum(((d1-d2)**2)/d2)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorDataFrameArtificial(x):\n",
    "    \n",
    "    c1 = 'background-color: forestgreen'\n",
    "    c2 = 'background-color: firebrick'\n",
    "    c = 'color: black'\n",
    "    cg = 'background-color: lightgrey'\n",
    "    cb = 'background-color: white'\n",
    "\n",
    "    # M2_D\n",
    "    m1_8 = (x.iloc[:,8] <= x.iloc[:,4]) \n",
    "    m2_8= (x.iloc[:,8] > x.iloc[:,4])\n",
    "\n",
    "    # M1_rN\n",
    "    m1_11 = (x.iloc[:,11] >= -0.05) & (x.iloc[:,11] <= 0.05)\n",
    "    m2_11 = (x.iloc[:,11] < -0.05) | (x.iloc[:,11] > 0.05)\n",
    "\n",
    "    # M2_rN\n",
    "    m1_12 = (x.iloc[:,12] >= x.iloc[:,10]*0.8) & (x.iloc[:,12] <= x.iloc[:,10]*1.2)\n",
    "    m2_12 = (x.iloc[:,12] < x.iloc[:,10]*0.8) | (x.iloc[:,12] > x.iloc[:,10]*1.2)\n",
    "\n",
    "    df = pd.DataFrame(cg,index=x.index,columns=x.columns)\n",
    "\n",
    "    df.iloc[:,8] = np.select([m1_8,m2_8],[c1,c2],default=cb)\n",
    "    df.iloc[:,11] = np.select([m1_11,m2_11],[c1,c2],default=cb)\n",
    "    df.iloc[:,12] = np.select([m1_12,m2_12],[c1,c2],default=cb)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorDataFrameUnmodified(x):\n",
    "    cg = 'background-color: lightgrey'\n",
    "    df = pd.DataFrame(cg,index=x.index,columns=x.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining(lossFunc,activationFunc,lossWeight,hiddenLayers,learningRate,batch_size,epochs,regCoeff,path):\n",
    "    import pandas as pd\n",
    "    # Clear Prior Model\n",
    "    import keras\n",
    "    keras.backend.clear_session()\n",
    "    # Create Model\n",
    "    model = createModel(hiddenLayers,activationFunc,regCoeff)\n",
    "    # Train Model\n",
    "    trainedModel = trainModel(model,noiseTrain_NN,cleanTrain_NN,lossWeight,batch_size,epochs,lossFunc,learningRate)\n",
    "    \n",
    "    # Save trained model\n",
    "    trainedModel.save(path+'/trainedModel')\n",
    "    \n",
    "    return trainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:46: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:67: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:79: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:91: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:112: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:46: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:67: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:79: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:91: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:112: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:46: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:67: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:79: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:91: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "<>:112: DeprecationWarning: invalid escape sequence \\h\n",
      "<ipython-input-174-bef2eea0ab46>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:32: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:46: DeprecationWarning: invalid escape sequence \\h\n",
      "  histName = ['$I\\'$','$A\\'4','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:53: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:67: DeprecationWarning: invalid escape sequence \\h\n",
      "  histName = ['$I\\'$','$A\\'$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:73: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:79: DeprecationWarning: invalid escape sequence \\h\n",
      "  histName = ['$I\\'$','$A\\'$','$\\hat{A}\\'$']\n",
      "<ipython-input-174-bef2eea0ab46>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$','QS $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
      "<ipython-input-174-bef2eea0ab46>:85: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$','QS $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
      "<ipython-input-174-bef2eea0ab46>:91: DeprecationWarning: invalid escape sequence \\h\n",
      "  histName = ['$P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
      "<ipython-input-174-bef2eea0ab46>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$','QS $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
      "<ipython-input-174-bef2eea0ab46>:98: DeprecationWarning: invalid escape sequence \\h\n",
      "  names = ['AR $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$','QS $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
      "<ipython-input-174-bef2eea0ab46>:112: DeprecationWarning: invalid escape sequence \\h\n",
      "  histName = ['$P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "def runTestingArtificial(trainedModel,lossFunc,activationFunc,lossWeight,hiddenLayers,df,C,dirPath,plotOn,channels=['94','131','171','193','211','304','335'],solarTypes=['AR','QS']):\n",
    "    \n",
    "    # Test Model\n",
    "    removedNoise_image, addedNoise_image, denoiseTest_image = testModelArtificial(trainedModel,cleanTest_NN,cleanTest_image,noiseTest_NN,noiseTest_image)\n",
    "\n",
    "    # Make new channels varible based on lossWeights\n",
    "    newChannels = [x for (x,y) in zip(channels,lossWeight) if y == 1 in lossWeight]\n",
    "        \n",
    "    ###\n",
    "    # Save generated images\n",
    "    ###\n",
    "    if plotOn == 1:\n",
    "        \n",
    "        cmaps=['sdoaia94','sdoaia131','sdoaia171','sdoaia193','sdoaia211','sdoaia304','sdoaia335']\n",
    "        # Make new cmap varible based on lossWeights\n",
    "        plotCmaps = [x for (x,y) in zip(cmaps,lossWeight) if y == 1 in lossWeight]\n",
    "        \n",
    "        # Make subdirectory\n",
    "        subdirName = lossFunc+'_'+activationFunc+'_'+str(lossWeight)+'_'+str(hiddenLayers)+'_Artificial'\n",
    "        subdirPath = dirPath+'/'+subdirName\n",
    "        if os.path.exists(subdirPath) == False:\n",
    "            os.mkdir(subdirPath)\n",
    "\n",
    "        ### Plot all images (zoomed in)\n",
    "        zoomStart_x_AR = 1100 \n",
    "        zoomStart_y_AR = 500 \n",
    "        zoomStart_x_QS = 200\n",
    "        zoomStart_y_QS = 500 \n",
    "        zoomSize = 150\n",
    "        names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
    "        #stacks = [rescale(np.stack(cleanTest_image)[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(noiseTest_image)[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(cleanTest_image)[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(noiseTest_image)[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize]]\n",
    "        stacks = [rescale(np.stack(cleanTest_image)[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(np.stack(noiseTest_image)[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(np.stack(cleanTest_image)[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(np.stack(noiseTest_image)[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize]]\n",
    "        file_name = 'denoise_zoomed-'+subdirName\n",
    "        histName = ['$I\\'$','$A\\'4','$\\hat{A}\\'$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath)\n",
    "\n",
    "        ### Plot all images (very zoomed in)\n",
    "        zoomSize = 50\n",
    "        names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
    "        #stacks = [rescale(np.stack(cleanTest_image)[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(noiseTest_image)[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(cleanTest_image)[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(noiseTest_image)[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize]]\n",
    "        stacks = [rescale(np.stack(cleanTest_image)[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(np.stack(noiseTest_image)[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(np.stack(cleanTest_image)[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(np.stack(noiseTest_image)[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize]]\n",
    "        file_name = 'denoise_very_zoomed-'+subdirName\n",
    "        histName = ['$I\\'$','$A\\'$','$\\hat{A}\\'$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath)\n",
    "        \n",
    "        ### Plot all images\n",
    "        names = ['AR $I\\'$','$A\\'$','$\\hat{A}\\'$','QS $I$','$A$','$\\hat{A}\\'$']\n",
    "        #stacks = [rescale(np.stack(cleanTest_image)[0],scaleStack),rescale(np.stack(noiseTest_image)[0],scaleStack),rescale(denoiseTest_image[0],scaleStack),\n",
    "        #          rescale(np.stack(cleanTest_image)[1],scaleStack),rescale(np.stack(noiseTest_image)[1],scaleStack),rescale(denoiseTest_image[1],scaleStack)]\n",
    "        stacks = [rescale(np.stack(cleanTest_image)[0],scaleStack),rescale(np.stack(noiseTest_image)[0],scaleStack),rescale(denoiseTest_image[0],scaleStack),\n",
    "                  rescale(np.stack(cleanTest_image)[1],scaleStack),rescale(np.stack(noiseTest_image)[1],scaleStack),rescale(denoiseTest_image[1],scaleStack)]\n",
    "        file_name = 'denoise-'+subdirName\n",
    "        histName = ['$I\\'$','$A\\'$','$\\hat{A}\\'$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath)\n",
    "\n",
    "        ### Plot all noise\n",
    "        names = ['AR $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$','QS $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
    "        #stacks = [level_1_testNoise[0],rescale(removedNoise_image[0],scaleStack),rescale(addedNoise_image[0],scaleStack),\n",
    "        #          level_1_testNoise[1],rescale(removedNoise_image[1],scaleStack),rescale(addedNoise_image[1],scaleStack)]\n",
    "        stacks = [level_1_testNoise[0],rescale(removedNoise_image[0],scaleStack),rescale(addedNoise_image[0],scaleStack),\n",
    "                  level_1_testNoise[0],rescale(removedNoise_image[0],scaleStack),rescale(addedNoise_image[0],scaleStack)]\n",
    "        file_name = 'noise-'+subdirName\n",
    "        histName = ['$P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath,cmaps=['seismic','seismic','seismic','seismic','seismic','seismic','seismic'])\n",
    "    \n",
    "    \n",
    "        ### Plot zoomed in all noise\n",
    "        names = ['AR $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$','QS $P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
    "        #stacks = [level_1_testNoise[0][:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(addedNoise_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          level_1_testNoise[1][:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(removedNoise_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(addedNoise_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize]]\n",
    "        stacks = [level_1_testNoise[0][:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(addedNoise_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  level_1_testNoise[0][:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(addedNoise_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize]]\n",
    "        file_name = 'noise_zoomed-'+subdirName\n",
    "        histName = ['$P$','$(A\\'-\\hat{A}\\')$','$(A\\'-I\\')$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath,cmaps=['seismic','seismic','seismic','seismic','seismic','seismic','seismic'])\n",
    "    ###\n",
    "    # Update statisitcs dataframe\n",
    "    ###\n",
    "    for i in range(len(channels)):\n",
    "        for j in range(len(solarTypes)):\n",
    "            #if lossWeight[j] == 1:\n",
    "            # Put everything in O,N,D notation and postprocess(rescale)\n",
    "            aN = addedNoise_image[j][i].flatten()\n",
    "            rN = removedNoise_image[j][i].flatten()\n",
    "            D = denoiseTest_image[j][i].flatten()\n",
    "            O = np.stack(cleanTest_image)[j][i].flatten()\n",
    "            N = np.stack(noiseTest_image)[j][i].flatten()\n",
    "\n",
    "            ### \n",
    "            # Statistics\n",
    "            ###\n",
    "\n",
    "            ### Added vs. Removed Noise \n",
    "            MSE_aNrN = np.mean(np.square(aN-rN)) # Mean Square Error\n",
    "            L1_loss_aNrN = np.mean(np.abs(aN-rN)) \n",
    "            chi2_aNrN = getChi2(aN,rN)\n",
    "\n",
    "            ### Original, Noisey, Denoised, Added, Removed \n",
    "            M1_O = np.mean(O) # First moment\n",
    "            M2_O = np.std(O) # Second moment\n",
    "            M1_N = np.mean(N) \n",
    "            M2_N = np.std(N) \n",
    "            M1_D = np.mean(D) \n",
    "            M2_D = np.std(D) \n",
    "            M1_aN = np.mean(aN) \n",
    "            M2_aN = np.std(aN) \n",
    "            M1_rN = np.mean(rN) \n",
    "            M2_rN = np.std(rN) \n",
    "\n",
    "            ### Original vs. Noisey & Denoised \n",
    "            #MSE_OD = np.mean(np.square(O-D))\n",
    "            #L1_loss_OD = np.mean(np.abs(O-D)) \n",
    "            #chi2_OD = getChi2(O,N)\n",
    "            #MSE_ON = np.mean(np.square(O-N))\n",
    "            #L1_loss_ON = np.mean(np.abs(O-N)) \n",
    "            #chi2_ON = getChi2(O,N)\n",
    "\n",
    "            configData = [lossFunc,activationFunc,str(lossWeight),str(hiddenLayers),channels[i],solarTypes[j]]\n",
    "            data = [MSE_aNrN,L1_loss_aNrN,chi2_aNrN,\n",
    "                    M1_O,M2_O,M1_N,M2_N,M1_D,M2_D,M1_aN,M2_aN,M1_rN,M2_rN]\n",
    "            df_intermediate = pd.DataFrame([configData+data],columns=C)\n",
    "            df = df.append(df_intermediate)       \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "def runTestingUnmodified(trainedModel,lossFunc,activationFunc,lossWeight,hiddenLayers,df,C,dirPath,plotOn,channels=['94','131','171','193','211','304','335'],solarTypes=['AR','QS']):\n",
    "    \n",
    "    # Test Model with recent data\n",
    "    removedNoise_image, denoiseTest_image = testModelUnmodified(trainedModel,cleanTestRecent_NN,cleanTestRecent_image)\n",
    "    \n",
    "    # Make new channels varible based on lossWeights\n",
    "    newChannels = [x for (x,y) in zip(channels,lossWeight) if y == 1 in lossWeight]\n",
    "        \n",
    "    ###\n",
    "    # Save generated images\n",
    "    ###\n",
    "    if plotOn == 1:\n",
    "        \n",
    "        cmaps=['sdoaia94','sdoaia131','sdoaia171','sdoaia193','sdoaia211','sdoaia304','sdoaia335']\n",
    "        # Make new cmap varible based on lossWeights\n",
    "        plotCmaps = [x for (x,y) in zip(cmaps,lossWeight) if y == 1 in lossWeight]\n",
    "        \n",
    "        # Make subdirectory\n",
    "        subdirName = lossFunc+'_'+activationFunc+'_'+str(lossWeight)+'_'+str(hiddenLayers)+'_Unmodified'\n",
    "        subdirPath = dirPath+'/'+subdirName\n",
    "        if os.path.exists(subdirPath) == False:\n",
    "            os.mkdir(subdirPath)\n",
    "\n",
    "        ### Plot all images (zoomed in)\n",
    "        zoomStart_x_AR = 900\n",
    "        zoomStart_y_AR = 500 \n",
    "        zoomStart_x_QS = 200\n",
    "        zoomStart_y_QS = 1200\n",
    "        zoomSize = 150\n",
    "        names = ['AR $I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$','QS $I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$']\n",
    "        #stacks = [rescale(np.stack(cleanTestRecent_image)[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(cleanTestRecent_image)[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(removedNoise_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize]]\n",
    "        stacks = [rescale(np.stack(cleanTestRecent_image)[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(np.stack(cleanTestRecent_image)[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize]]\n",
    "        file_name = 'denoise_zoomed-'+subdirName\n",
    "        histName = ['$I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath,testType = 0)\n",
    "\n",
    "        ### Plot all images (very zoomed in)\n",
    "        zoomSize = 50\n",
    "        names = ['AR I\\'','$\\hat{I}$\\'','(I\\'-$\\hat{I}$\\')','QS I\\'','$\\hat{I}$\\'','(I\\'-$\\hat{I}$\\')']\n",
    "        #stacks = [rescale(np.stack(cleanTestRecent_image)[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(np.stack(cleanTestRecent_image)[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(denoiseTest_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize],\n",
    "        #          rescale(removedNoise_image[1],scaleStack)[:,zoomStart_x:zoomStart_x+zoomSize,zoomStart_y:zoomStart_y+zoomSize]]\n",
    "        stacks = [rescale(np.stack(cleanTestRecent_image)[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x_AR:zoomStart_x_AR+zoomSize,zoomStart_y_AR:zoomStart_y_AR+zoomSize],\n",
    "                  rescale(np.stack(cleanTestRecent_image)[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(denoiseTest_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize],\n",
    "                  rescale(removedNoise_image[0],scaleStack)[:,zoomStart_x_QS:zoomStart_x_QS+zoomSize,zoomStart_y_QS:zoomStart_y_QS+zoomSize]]\n",
    "        file_name = 'denoise_very_zoomed-'+subdirName\n",
    "        histName = ['$I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath,testType = 0)\n",
    "        \n",
    "        ### Plot all images\n",
    "        names = ['AR $I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$','QS $I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$']\n",
    "        #stacks = [rescale(np.stack(cleanTestRecent_image)[0],scaleStack),\n",
    "        #          rescale(denoiseTest_image[0],scaleStack),\n",
    "        #          rescale(removedNoise_image[0],scaleStack),\n",
    "        #          rescale(np.stack(cleanTestRecent_image)[1],scaleStack),\n",
    "        #          rescale(denoiseTest_image[1],scaleStack),\n",
    "        #          rescale(removedNoise_image[1],scaleStack)]\n",
    "        stacks = [rescale(np.stack(cleanTestRecent_image)[0],scaleStack),\n",
    "                  rescale(denoiseTest_image[0],scaleStack),\n",
    "                  rescale(removedNoise_image[0],scaleStack),\n",
    "                  rescale(np.stack(cleanTestRecent_image)[1],scaleStack),\n",
    "                  rescale(denoiseTest_image[1],scaleStack),\n",
    "                  rescale(removedNoise_image[1],scaleStack)]\n",
    "        file_name = 'denoise-'+subdirName\n",
    "        histName = ['$I\\'$','$\\hat{I}\\'$','$(I\\'-\\hat{I}\\')$']\n",
    "        saveFigure = 1\n",
    "        bins = 50\n",
    "        stackPlotter(names,stacks,file_name,histName,saveFigure,bins,subdirPath,testType = 0)\n",
    "\n",
    "    ###\n",
    "    # Update statisitcs dataframe\n",
    "    ###\n",
    "    for i in range(len(channels)):\n",
    "        for j in range(len(solarTypes)):\n",
    "            #if lossWeight[j] == 1:\n",
    "            # Put everything in O,N,D notation and postprocess(rescale)\n",
    "            rN = removedNoise_image[j][i].flatten()\n",
    "            D = denoiseTest_image[j][i].flatten()\n",
    "            O = np.stack(cleanTestRecent_image)[j][i].flatten()\n",
    "\n",
    "            ### \n",
    "            # Statistics\n",
    "            ###\n",
    "\n",
    "            ### Original vs. Denoise\n",
    "            MSE_OD = np.mean(np.square(O-D)) # Mean Square Error\n",
    "            L1_loss_OD = np.mean(np.abs(O-D)) \n",
    "            chi2_OD = getChi2(O,D)\n",
    "\n",
    "            ### Original, Noisey, Denoised, Added, Removed \n",
    "            M1_O = np.mean(O) # First moment\n",
    "            M2_O = np.std(O) # Second moment\n",
    "            M1_D = np.mean(D) \n",
    "            M2_D = np.std(D) \n",
    "            M1_rN = np.mean(rN) \n",
    "            M2_rN = np.std(rN) \n",
    "\n",
    "            configData = [lossFunc,activationFunc,str(lossWeight),str(hiddenLayers),channels[i],solarTypes[j]]\n",
    "            data = [MSE_OD,L1_loss_OD,chi2_OD,\n",
    "                    M1_O,M2_O,M1_D,M2_D,M1_rN,M2_rN]\n",
    "            df_intermediate = pd.DataFrame([configData+data],columns=C)\n",
    "            df = df.append(df_intermediate)       \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 7)            56          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            48          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            28          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            30          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            49          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 7)            0           dense_4[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 211\n",
      "Trainable params: 211\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "356125/356125 [==============================] - 300s 843us/step - loss: 0.1569 - val_loss: 0.1213\n",
      "Epoch 2/25\n",
      "356125/356125 [==============================] - 303s 851us/step - loss: 0.1507 - val_loss: 0.1209\n",
      "Epoch 3/25\n",
      "356125/356125 [==============================] - 297s 835us/step - loss: 0.1501 - val_loss: 0.1209\n",
      "Epoch 4/25\n",
      "356125/356125 [==============================] - 294s 825us/step - loss: 0.1499 - val_loss: 0.1207\n",
      "Epoch 5/25\n",
      "356125/356125 [==============================] - 293s 823us/step - loss: 0.1497 - val_loss: 0.1206\n",
      "Epoch 6/25\n",
      "356125/356125 [==============================] - 294s 825us/step - loss: 0.1495 - val_loss: 0.1206\n",
      "Epoch 7/25\n",
      "356125/356125 [==============================] - 295s 827us/step - loss: 0.1494 - val_loss: 0.1205\n",
      "Epoch 8/25\n",
      "356125/356125 [==============================] - 295s 828us/step - loss: 0.1492 - val_loss: 0.1200\n",
      "Epoch 9/25\n",
      "356125/356125 [==============================] - 295s 829us/step - loss: 0.1489 - val_loss: 0.1197\n",
      "Epoch 10/25\n",
      "356125/356125 [==============================] - 296s 831us/step - loss: 0.1487 - val_loss: 0.1195\n",
      "Epoch 11/25\n",
      "356125/356125 [==============================] - 295s 829us/step - loss: 0.1486 - val_loss: 0.1196\n",
      "Epoch 12/25\n",
      "356125/356125 [==============================] - 295s 829us/step - loss: 0.1485 - val_loss: 0.1194\n",
      "Epoch 13/25\n",
      "356125/356125 [==============================] - 294s 826us/step - loss: 0.1485 - val_loss: 0.1194\n",
      "Epoch 14/25\n",
      "356125/356125 [==============================] - 294s 825us/step - loss: 0.1485 - val_loss: 0.1194\n",
      "Epoch 15/25\n",
      "356125/356125 [==============================] - 294s 826us/step - loss: 0.1485 - val_loss: 0.1194\n",
      "Epoch 16/25\n",
      "356125/356125 [==============================] - 299s 839us/step - loss: 0.1485 - val_loss: 0.1193\n",
      "Epoch 17/25\n",
      "356125/356125 [==============================] - 294s 824us/step - loss: 0.1485 - val_loss: 0.1194\n",
      "Epoch 18/25\n",
      "356125/356125 [==============================] - 295s 828us/step - loss: 0.1485 - val_loss: 0.1193\n",
      "Epoch 19/25\n",
      "356125/356125 [==============================] - 296s 830us/step - loss: 0.1484 - val_loss: 0.1194\n",
      "Epoch 20/25\n",
      "356125/356125 [==============================] - 295s 829us/step - loss: 0.1484 - val_loss: 0.1193\n",
      "Epoch 21/25\n",
      "356125/356125 [==============================] - 297s 833us/step - loss: 0.1484 - val_loss: 0.1194\n",
      "Epoch 22/25\n",
      "356125/356125 [==============================] - 297s 833us/step - loss: 0.1484 - val_loss: 0.1193\n",
      "Epoch 23/25\n",
      "356125/356125 [==============================] - 295s 829us/step - loss: 0.1484 - val_loss: 0.1194\n",
      "Epoch 24/25\n",
      "356125/356125 [==============================] - 296s 832us/step - loss: 0.1484 - val_loss: 0.1193\n",
      "Epoch 25/25\n",
      "356125/356125 [==============================] - 295s 829us/step - loss: 0.1484 - val_loss: 0.1193\n",
      "INFO:tensorflow:Assets written to: /sanhome/penwarden/public_html/13th_Testing_overnight_B25_E25/trainedModel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Main()\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "### Configurations\n",
    "#batch_size = 5\n",
    "batch_size = 25\n",
    "epochs = 25\n",
    "#epochs = 15\n",
    "learningRate = 0.0001\n",
    "plotOn = 1\n",
    "importDF = 0\n",
    "regCoeff = 0 #Regularization coefficent for activations on hidden layers (used for sparse autoencoder)\n",
    "\n",
    "activationFunctions = ['LeakyReLU'] # Options are ReLU, LeakyReLU, Linear\n",
    "\n",
    "lossWeights =[[1,1,1,1,1,1,1]]\n",
    "\n",
    "hiddenLayers = [[6,4,6]]\n",
    "\n",
    "lossFunctions = ['L1'] # Options are MSE & L1\n",
    "\n",
    "path = '/sanhome/penwarden/public_html/'\n",
    "dirName = '13th_Testing_overnight_B25_E25'\n",
    "dirPath = path+dirName\n",
    "testType = 2 # 2 for both, 1 is for the artificially noisy images, 0 is for the unmodified recent images\n",
    "###\n",
    "\n",
    "# Create Test directory \n",
    "if os.path.exists(dirPath) == False:\n",
    "    os.mkdir(dirPath)\n",
    "\n",
    "layersString = []\n",
    "for i in range(len(hiddenLayers)):\n",
    "    layersString.append(str(hiddenLayers[i]))\n",
    "\n",
    "\n",
    "    # Initialize dataframe\n",
    "    configList = ['Loss Function']+['Activation Function']+['Loss Weights']+['Hidden Layers Config']+['Channel']+['Solar Type']\n",
    "\n",
    "    if testType == 1 or testType == 2:\n",
    "        statisticsList_artificial = ['MSE_aNrN','L1_loss_aNrN','Chi2_aNrN',\n",
    "                        'M1_O','M2_O','M1_N','M2_N','M1_D','M2_D','M1_aN','M2_aN','M1_rN','M2_rN']\n",
    "        C_artificial = pd.Index(configList+statisticsList_artificial)\n",
    "    if testType == 0 or testType == 2:\n",
    "        statisticsList_unmodified = ['MSE_OD','L1_loss_OD','Chi2_OD',\n",
    "                        'M1_O','M2_O','M1_D','M2_D','M1_rN','M2_rN']\n",
    "        C_unmodified = pd.Index(configList+statisticsList_unmodified)\n",
    "    \n",
    "if importDF == 0:\n",
    "    if testType == 1 or testType == 2:\n",
    "        df_artificial = pd.DataFrame(columns=C_artificial)\n",
    "    if testType == 0 or testType == 2:\n",
    "        df_unmodified = pd.DataFrame(columns=C_unmodified)\n",
    "else: \n",
    "    if testType == 1 or testType == 2:\n",
    "        df_artificial = pd.read_pickle(dirPath+'/'+dirName+'_artificial.pkl')\n",
    "    if testType == 0 or testType == 2:\n",
    "        df_unmodified = pd.read_pickle(dirPath+'/'+dirName+'_unmodified.pkl')\n",
    "\n",
    "for i in range(len(lossFunctions)):\n",
    "    for j in range(len(activationFunctions)):\n",
    "        for k in range(len(lossWeights)):\n",
    "            for l in range(len(hiddenLayers)):\n",
    "                trainedModel = runTraining(lossFunctions[i],activationFunctions[j],lossWeights[k],hiddenLayers[l],learningRate,batch_size,epochs,regCoeff,dirPath)\n",
    "                if testType == 1 or testType == 2:\n",
    "                    df_artificial = runTestingArtificial(trainedModel,lossFunctions[i],activationFunctions[j],lossWeights[k],hiddenLayers[l],df_artificial,C_artificial,dirPath,plotOn)\n",
    "                if testType == 0 or testType == 2:\n",
    "                    df_unmodified = runTestingUnmodified(trainedModel,lossFunctions[i],activationFunctions[j],lossWeights[k],hiddenLayers[l],df_unmodified,C_unmodified,dirPath,plotOn)\n",
    "             \n",
    "if testType == 1 or testType == 2:\n",
    "    # Make saves\n",
    "    dfUnstyled_artificial = df_artificial.copy()\n",
    "    dfUnstyled_artificial.to_pickle(dirPath+'/'+dirName+'_artificial.pkl') # Save the dataframe without styler for future use\n",
    "    dfUnstyled_artificial.to_csv(dirPath+'/'+dirName+'_artificial.csv') # Save csv just in case\n",
    "\n",
    "    # Make Hierarchical Indices\n",
    "    df_artificial = df_artificial.set_index(['Loss Function','Activation Function','Loss Weights','Hidden Layers Config','Channel','Solar Type'])\n",
    "    df_artificial = df_artificial.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Add color\n",
    "    df_artificial = df_artificial.style.apply(colorDataFrameArtificial,axis=None)\\\n",
    "    .background_gradient('RdYlGn_r',axis=0,subset=['MSE_aNrN'], vmin = 0, vmax = 2, text_color_threshold = 0)\\\n",
    "    .background_gradient('RdYlGn_r',axis=0,subset=['L1_loss_aNrN'], vmin = 0, vmax = 2,text_color_threshold = 0)\\\n",
    "    .background_gradient('RdYlGn_r',axis=0,subset=['Chi2_aNrN'], vmin = 0, vmax = 100, text_color_threshold = 0)\n",
    "        \n",
    "    # Write to file\n",
    "    df_artificial.to_excel(dirPath+'/'+dirName+'_artificial.xlsx',float_format='%.3f') # Write to excel file to retain dataframe hierarchical indexing and coloring\n",
    "    with open(dirPath+'/'+dirName+'_artificial.html', 'w') as fo:\n",
    "        fo.write(df_artificial.render())\n",
    "        \n",
    "if testType == 0 or testType == 2:  \n",
    "    # Make saves\n",
    "    dfUnstyled_unmodified = df_unmodified.copy()\n",
    "    dfUnstyled_unmodified.to_pickle(dirPath+'/'+dirName+'_unmodified.pkl') # Save the dataframe without styler for future use\n",
    "    dfUnstyled_unmodified.to_csv(dirPath+'/'+dirName+'_unmodified.csv') # Save csv just in case\n",
    "\n",
    "    # Make Hierarchical Indices\n",
    "    df_unmodified = df_unmodified.set_index(['Loss Function','Activation Function','Loss Weights','Hidden Layers Config','Channel','Solar Type'])\n",
    "    df_unmodified = df_unmodified.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Add color\n",
    "    df_unmodified = df_unmodified.style.apply(colorDataFrameUnmodified,axis=None)\n",
    "    \n",
    "    # Write to file\n",
    "    df_unmodified.to_excel(dirPath+'/'+dirName+'_unmodified.xlsx',float_format='%.3f') # Write to excel file to retain dataframe hierarchical indexing and coloring\n",
    "    with open(dirPath+'/'+dirName+'_unmodified.html', 'w') as fo:\n",
    "        fo.write(df_unmodified.render())\n",
    "        \n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
